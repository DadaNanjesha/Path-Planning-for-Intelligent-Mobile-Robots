# ğŸš€ Advanced Path Planning Algorithms for Intelligent Mobile Robots

## ğŸ“Œ Overview
This repository contains implementations of various **advanced path-planning algorithms** used in autonomous mobile robots. These algorithms are widely applied in **robotics, AI, and autonomous navigation systems**. 

The repository includes implementations of:
- **A\***, **D\***, **Hybrid DQN-A\***, **RRT\***, and **Theta\*** algorithms.
- Efficient solutions for **navigation in dynamic and static environments**.

---

## âœ¨ Features
âœ” **A\* Algorithm** - Implements the classical A\* search and bidirectional variant for optimal path planning.  
âœ” **D\* Algorithm** - A dynamic path planning algorithm that adapts to changing environments.  
âœ” **Hybrid DQN-A\*** - A hybrid approach combining Deep Q-Network (DQN) with A\* for intelligent decision-making.  
âœ” **RRT\* Algorithm** - Rapidly-exploring Random Tree (RRT\*) for sampling-based path planning.  
âœ” **Theta\* Algorithm** - An optimized variant of A\* allowing any-angle path planning.  

---

## ğŸ“‚ Directory Structure
```plaintext
ğŸ“¦ Advanced_Path_Planning
â”œâ”€â”€ ğŸ“‚ A_Star
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ a_star_algorithm.py
â”‚   â”œâ”€â”€ ğŸ“„ a_star_bidirectional_algorithm.py
â”‚
â”œâ”€â”€ ğŸ“‚ D_Star
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ d_star.py
â”‚
â”œâ”€â”€ ğŸ“‚ Hybrid_DQN_A_Star
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ hybrid_dqn_a_star_algorithm.py
â”‚
â”œâ”€â”€ ğŸ“‚ RRT_Star
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ rrt_star_algorithm.py
â”‚
â”œâ”€â”€ ğŸ“‚ Theta_Star
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ theta_star_algorithm.py
â”‚
â”œâ”€â”€ ğŸ“‚ media
â”‚   â”œâ”€â”€ ğŸ“¹ astar.gif
â”‚   â”œâ”€â”€ ğŸ“¹ astar_bidirectional.gif
â”‚   â”œâ”€â”€ ğŸ“¹ dstar.gif
â”‚   â”œâ”€â”€ ğŸ“¹ dqn_astar.gif
â”‚   â”œâ”€â”€ ğŸ“¹ rrtstar.gif
â”‚   â”œâ”€â”€ ğŸ“¹ thetastar.gif
â”‚
â”œâ”€â”€ ğŸ“‚ test
â”‚   â”œâ”€â”€ ğŸ“„ test_astar.py
â”‚   â”œâ”€â”€ ğŸ“„ test_dstar.py
â”‚   â”œâ”€â”€ ğŸ“„ test_hybrid_dqn_a_star.py
â”‚   â”œâ”€â”€ ğŸ“„ test_rrt_star.py
â”‚   â”œâ”€â”€ ğŸ“„ test_theta_star.py
â”‚
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ requirements.txt
â”œâ”€â”€ ğŸ“„ setup.py
â”œâ”€â”€ ğŸ“‚ .github

```

---

## âš™ï¸ Installation
To run the algorithms, install the necessary dependencies:
```sh
pip install -r requirements.txt
pip install -e .

```

---

## ğŸš€ Usage
Each algorithm can be executed independently. Example usage:
```sh
python A_Star/a_star_algorithm.py
            or
pytest -v --verbose
```
Modify the scripts as needed to test different environments or configurations.

---

## ğŸ“Š Project Overview
The project contains several advanced path planning techniques, including:
### ğŸ”¹ **A\* Algorithm**
A classic grid-based path planning method that computes the optimal path using an 8-connected graph and a Euclidean distance heuristic.  
<img src="media/astar.gif" width="300" height="300" alt="A* Algorithm">

### ğŸ”¹ **Bidirectional A\***
An enhanced version of A\* that simultaneously searches from the start and goal, potentially reducing the search time.  
<img src="media/astar_bidirectional.gif" width="300" height="300" alt="Bidirectional A* Algorithm">

### ğŸ”¹ **D\* Algorithm**
A dynamic path planning algorithm that efficiently updates the optimal path in response to changes in the environment.  
<img src="media/dstar.gif" width="300" height="300" alt="D* Algorithm">

### ğŸ”¹ **RRT\* Algorithm**
A rapidly exploring random tree (RRT) is an algorithm designed to efficiently search nonconvex, high-dimensional spaces by randomly building a space-filling tree.  
<img src="media/rrtstar.gif" width="300" height="300" alt="D* Algorithm">

### ğŸ”¹ **Theta\* Algorithm**
An any-angle variant of A\* that uses line-of-sight checks to "shortcut" unnecessary nodes, producing smoother and more direct paths.  
<img src="media/thetastar.gif" width="300" height="300" alt="Theta* Algorithm">

### ğŸ”¹ **Advanced AI/ML (DQN-Based) Path Planning**
An implementation leveraging deep reinforcement learning (DQN) to learn an optimal navigation policy.  
<img src="media/dqn_astar.gif" width="300" height="300" alt="DQN-A* Algorithm">

---

## ğŸ¥ Visualization
Pathfinding results can be visualized using the generated `.gif` files in the `media` directory.

---

# **Final Performance Comparison Table**

This table provides a performance comparison of various path-planning algorithms implemented in this project.

| Algorithm          | Execution Time (s) | Path Length | Steps Taken | Direction Changes | Optimality | Computational Efficiency | Scalability | Adaptability to Dynamic Environments | Best Use Cases |
|--------------------|-------------------|-------------|-------------|-------------------|------------|---------------------------|-------------|--------------------------------------|----------------|
| A*                | 20.45 - 30.48      | 85.05       | 67          | 13                | High       | Low                       | Low         | Low                                  | Structured static environments (e.g., warehouse automation) |
| Bidirectional A*  | 4.20 - 5.46        | 96.77       | 87          | 7                 | Moderate   | High                      | Moderate    | High                                 | Dynamic environments with known obstacles |
| D*               | 20.50 - 22.11      | 85.05       | 67          | 13                | High       | Low                       | Low         | Moderate                             | Real-time replanning in semi-dynamic environments |
| RRT*             | 8.03 - 39.19       | 104.69 - 113.35 | 22 - 24   | 21 - 22           | Low - Moderate | Moderate              | High        | High                                 | Exploration & high-dimensional navigation |
| Theta*           | 0.08 - 0.12        | 82.27       | 6           | 4                 | Very High  | Very High                 | High        | Low                                  | Precise trajectory planning & smooth motion |
| DQN-A*           | 0.01               | 85.05       | 67          | 13                | High       | Very High                 | Very High   | Very High                            | AI-enhanced real-time obstacle avoidance |

This comparison helps in selecting the appropriate algorithm based on the application requirements, balancing optimality, efficiency, and adaptability.


## ğŸ¤ Contributing
Contributions are welcome! To contribute:
1. Fork the repository
2. Create a new branch (`git checkout -b feature-branch`)
3. Commit changes (`git commit -m "Add new feature"`)
4. Push to the branch (`git push origin feature-branch`)
5. Open a Pull Request

---

## ğŸ“œ License
This project is licensed under the **MIT License** - see the LICENSE file for details.

---

## ğŸ“¬ Contact
For inquiries or collaborations, reach out via **GitHub Issues**.

---

ğŸ‰ **Happy Coding!** ğŸš€

