# ğŸš€ Hybrid DQN-A* Path Planner

The **Hybrid DQN-A* Path Planner** is an innovative path planning algorithm that combines **Deep Q-Network (DQN)** for reinforcement learning-based navigation with **A* search** as a fallback to guarantee successful pathfinding. This ensures the system benefits from both adaptive learning and reliable deterministic search.

---

## ğŸ“Œ Overview
ğŸ”¹ **Grid-Based Graph Construction** â€“ Environment is discretized into a grid, with obstacles removed based on clearance constraints.  
ğŸ”¹ **Deep Q-Network (DQN) Learning** â€“ Learns an optimal navigation policy using reinforcement learning.  
ğŸ”¹ **A* Search Fallback** â€“ Ensures a path is always computed if DQN fails.  
ğŸ”¹ **Dynamic Visualization** â€“ Real-time plotting of the search process using `matplotlib`.  
ğŸ”¹ **Performance Metrics** â€“ Reports execution time, path length, steps, and direction changes.  

---

## âœ¨ Features

### ğŸ— **1. Grid Graph Construction**
âœ” Builds a structured grid for motion planning.  
âœ” **Obstacle Handling** â€“ Nodes within a defined clearance radius are removed.  

---

### ğŸ”„ **2. 8-Connected Motion Model**
âœ” Supports **8 movement directions**: 
   - **Cardinal Directions** (Up, Down, Left, Right) â†’ Cost = 1
   - **Diagonal Movements** (Top-Left, Top-Right, Bottom-Left, Bottom-Right) â†’ Cost = âˆš2

Cost function for movement:


$$
\text{Cost}(dx, dy) = 
\begin{cases} 
1 & \text{if } dx \text{ or } dy = 0 \text{ (cardinal)} \\
\sqrt{2} & \text{if } dx \neq 0 \text{ and } dy \neq 0 \text{ (diagonal)}
\end{cases}
$$



---

### ğŸ¤– **3. Deep Q-Network (DQN) Training**
âœ” Uses a **feedforward neural network** to approximate Q-values.  
âœ” Learns the best action policy using the **epsilon-greedy strategy**.  
âœ” Trained using the **Bellman equation**:
```math
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
```
where:

  - ${\alpha}$ is the learning rate,
  - ${\gamma}$ is the discount factor,
  - ${r_t}$  is the reward,
  - $s_{t+1}$   is the next state.

---

### â­ **4. A* Search Fallback**
âœ” If DQN **fails to reach the goal**, A* is used to complete the path.  
âœ” Uses the **Euclidean distance heuristic**:
```math
h(n_1, n_2) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
```
where \( (x_1, y_1) \) and \( (x_2, y_2) \) are the node coordinates.

---

### ğŸ¥ **5. Dynamic Visualization**
âœ” Real-time plotting of search progress using `matplotlib`.  
âœ” **0.5-second pause** between points to visualize movement.

---

### ğŸ“Š **6. Performance Metrics**
âœ” **Execution Time** â€“ Total time taken for path computation.  
âœ” **Path Length** â€“ Total Euclidean distance from start to goal.  
âœ” **Steps Taken** â€“ Number of nodes in the computed path.  
âœ” **Direction Changes** â€“ Number of times the path switches between movement types.

---

## ğŸ‹ï¸â€â™‚ï¸ Training & Evaluation

### ğŸ **1. Training the DQN Agent**
âœ” **State Representation**:
```math
\text{state} = [\text{current}_x, \text{current}_y, \text{goal}_x, \text{goal}_y]
```
âœ” **Action Space**: 8 possible movement actions.  
âœ” **Reward Structure**:
   - Positive reward for reaching goal.
   - Negative penalty for collisions.

---

### ğŸ“Š **2. Evaluation**
âœ” **Policy Execution** â€“ Checks if the trained DQN agent reaches the goal.  
âœ” **Fallback Mechanism** â€“ A* search is triggered when necessary.  

---

ğŸ‰ **Happy Path Planning! ğŸš€**

